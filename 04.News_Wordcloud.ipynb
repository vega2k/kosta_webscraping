{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: feedparser in c:\\anaconda3\\lib\\site-packages (6.0.2)\n",
      "Requirement already satisfied: sgmllib3k in c:\\anaconda3\\lib\\site-packages (from feedparser) (1.0.0)\n"
     ]
    }
   ],
   "source": [
    "#파이썬의 RSS 파서인 feedparser\n",
    "!pip install feedparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting newspaper3k\n",
      "  Using cached newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
      "Collecting feedfinder2>=0.0.4\n",
      "  Using cached feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
      "Requirement already satisfied: PyYAML>=3.11 in c:\\anaconda3\\lib\\site-packages (from newspaper3k) (5.3.1)\n",
      "Collecting tldextract>=2.0.1\n",
      "  Downloading tldextract-3.1.0-py2.py3-none-any.whl (87 kB)\n",
      "Collecting tinysegmenter==0.3\n",
      "  Using cached tinysegmenter-0.3.tar.gz (16 kB)\n",
      "Collecting jieba3k>=0.35.1\n",
      "  Using cached jieba3k-0.35.1.zip (7.4 MB)\n",
      "Requirement already satisfied: lxml>=3.6.0 in c:\\anaconda3\\lib\\site-packages (from newspaper3k) (4.6.1)\n",
      "Requirement already satisfied: nltk>=3.2.1 in c:\\anaconda3\\lib\\site-packages (from newspaper3k) (3.5)\n",
      "Collecting feedparser>=5.2.1\n",
      "  Downloading feedparser-6.0.2-py3-none-any.whl (80 kB)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\anaconda3\\lib\\site-packages (from newspaper3k) (2.24.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in c:\\anaconda3\\lib\\site-packages (from newspaper3k) (2.8.1)\n",
      "Collecting cssselect>=0.9.2\n",
      "  Using cached cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.4.1 in c:\\anaconda3\\lib\\site-packages (from newspaper3k) (4.9.3)\n",
      "Requirement already satisfied: Pillow>=3.3.0 in c:\\anaconda3\\lib\\site-packages (from newspaper3k) (8.0.1)\n",
      "Requirement already satisfied: six in c:\\anaconda3\\lib\\site-packages (from feedfinder2>=0.0.4->newspaper3k) (1.15.0)\n",
      "Collecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: idna in c:\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (2.10)\n",
      "Requirement already satisfied: filelock>=3.0.8 in c:\\anaconda3\\lib\\site-packages (from tldextract>=2.0.1->newspaper3k) (3.0.12)\n",
      "Requirement already satisfied: click in c:\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (7.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (4.50.2)\n",
      "Requirement already satisfied: joblib in c:\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (0.17.0)\n",
      "Requirement already satisfied: regex in c:\\anaconda3\\lib\\site-packages (from nltk>=3.2.1->newspaper3k) (2020.10.15)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests>=2.10.0->newspaper3k) (1.25.11)\n",
      "Requirement already satisfied: soupsieve>1.2; python_version >= \"3.0\" in c:\\anaconda3\\lib\\site-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.0.1)\n",
      "Building wheels for collected packages: feedfinder2, tinysegmenter, jieba3k, sgmllib3k\n",
      "  Building wheel for feedfinder2 (setup.py): started\n",
      "  Building wheel for feedfinder2 (setup.py): finished with status 'done'\n",
      "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3362 sha256=0a7b12a1e742f091602aa5b74ba77b5aecbbeca638ab0f671d63ff5d7efcee9c\n",
      "  Stored in directory: c:\\users\\vega2k\\appdata\\local\\pip\\cache\\wheels\\b6\\09\\68\\a9f15498ac02c23dde29f18745bc6a6f574ba4ab41861a3575\n",
      "  Building wheel for tinysegmenter (setup.py): started\n",
      "  Building wheel for tinysegmenter (setup.py): finished with status 'done'\n",
      "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13543 sha256=8c6b629e5a9fb18e0f49a6f845503742d8cfbc09152cf63d487956f2ac1ac03a\n",
      "  Stored in directory: c:\\users\\vega2k\\appdata\\local\\pip\\cache\\wheels\\99\\74\\83\\8fac1c8d9c648cfabebbbffe97a889f6624817f3aa0bbe6c09\n",
      "  Building wheel for jieba3k (setup.py): started\n",
      "  Building wheel for jieba3k (setup.py): finished with status 'done'\n",
      "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398411 sha256=e59fbe05066c14fb365335994dff1c16dbcfff94ab394fd171fce5254dbc0420\n",
      "  Stored in directory: c:\\users\\vega2k\\appdata\\local\\pip\\cache\\wheels\\1f\\7e\\0c\\54f3b0f5164278677899f2db08f2b07943ce2d024a3c862afb\n",
      "  Building wheel for sgmllib3k (setup.py): started\n",
      "  Building wheel for sgmllib3k (setup.py): finished with status 'done'\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6070 sha256=736cdb846ba7554f32039903b452bba06c81a78f9332186e4104f179ac0b1740\n",
      "  Stored in directory: c:\\users\\vega2k\\appdata\\local\\pip\\cache\\wheels\\83\\63\\2f\\117884c3b19d46b64d3d61690333aa80c88dc14050e269c546\n",
      "Successfully built feedfinder2 tinysegmenter jieba3k sgmllib3k\n",
      "Installing collected packages: feedfinder2, requests-file, tldextract, tinysegmenter, jieba3k, sgmllib3k, feedparser, cssselect, newspaper3k\n",
      "Successfully installed cssselect-1.1.0 feedfinder2-0.0.4 feedparser-6.0.2 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-1.5.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-3.1.0\n"
     ]
    }
   ],
   "source": [
    "#newspaper는 사용자가 지정한 url에서 text를 추출해주는 모듈\n",
    "!pip install newspaper3k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp38-cp38-win_amd64.whl (155 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (from wordcloud) (3.3.2)\n",
      "Requirement already satisfied: pillow in c:\\anaconda3\\lib\\site-packages (from wordcloud) (8.0.1)\n",
      "Requirement already satisfied: numpy>=1.6.1 in c:\\anaconda3\\lib\\site-packages (from wordcloud) (1.19.2)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (1.3.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in c:\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2020.06.20 in c:\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (2020.6.20)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.1->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import feedparser\n",
    "import newspaper\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'feedparser.util.FeedParserDict'>\n",
      "dict_keys(['bozo', 'entries', 'feed', 'headers', 'href', 'status', 'encoding', 'version', 'namespaces'])\n",
      "<class 'list'>\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'subtitle', 'subtitle_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'subtitle', 'subtitle_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'subtitle', 'subtitle_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'subtitle', 'subtitle_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'subtitle', 'subtitle_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'subtitle', 'subtitle_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n",
      "dict_keys(['title', 'title_detail', 'links', 'link', 'published', 'published_parsed', 'summary', 'summary_detail', 'media_content'])\n",
      "------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['https://www.donga.com/news/Entertainment/article/all/20210428/106644659/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210428/106644653/1',\n",
       " 'https://www.donga.com/news/Entertainment/article/all/20210428/106644648/1',\n",
       " 'https://www.donga.com/news/Entertainment/article/all/20210428/106644641/1',\n",
       " 'https://www.donga.com/news/Entertainment/article/all/20210428/106644635/1',\n",
       " 'https://www.donga.com/news/Entertainment/article/all/20210428/106644628/1',\n",
       " 'https://www.donga.com/news/Entertainment/article/all/20210428/106644621/1',\n",
       " 'https://www.donga.com/news/Opinion/article/all/20210427/106644287/1',\n",
       " 'https://www.donga.com/news/Opinion/article/all/20210427/106644251/1',\n",
       " 'https://www.donga.com/news/Opinion/article/all/20210427/106644214/1',\n",
       " 'https://www.donga.com/news/Politics/article/all/20210427/106644471/1',\n",
       " 'https://www.donga.com/news/Inter/article/all/20210427/106635326/2',\n",
       " 'https://www.donga.com/news/It/article/all/20210427/106644293/1',\n",
       " 'https://www.donga.com/news/Inter/article/all/20210427/106644178/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106644154/1',\n",
       " 'https://www.donga.com/news/Inter/article/all/20210427/106644077/1',\n",
       " 'https://www.donga.com/news/Culture/article/all/20210427/106643980/1',\n",
       " 'https://www.donga.com/news/Politics/article/all/20210427/106643934/1',\n",
       " 'https://www.donga.com/news/Politics/article/all/20210427/106643872/1',\n",
       " 'https://www.donga.com/news/Inter/article/all/20210427/106635385/2',\n",
       " 'https://www.donga.com/news/Economy/article/all/20210427/106643579/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106643545/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106643531/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106643379/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106643368/2',\n",
       " 'https://www.donga.com/news/Inter/article/all/20210427/106640418/2',\n",
       " 'https://www.donga.com/news/list/article/all/20210427/106643300/2',\n",
       " 'https://www.donga.com/news/It/article/all/20210427/106643159/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106642957/1',\n",
       " 'https://www.donga.com/news/Sports/article/all/20210427/106642950/1',\n",
       " 'https://www.donga.com/news/Culture/article/all/20210427/106642948/1',\n",
       " 'https://www.donga.com/news/Politics/article/all/20210427/106642936/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106642851/2',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106642831/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106642773/1',\n",
       " 'https://www.donga.com/news/Inter/article/all/20210427/106628719/2',\n",
       " 'https://www.donga.com/news/Politics/article/all/20210427/106642565/1',\n",
       " 'https://www.donga.com/news/Culture/article/all/20210427/106642369/1',\n",
       " 'https://www.donga.com/news/Sports/article/all/20210427/106642356/2',\n",
       " 'https://www.donga.com/news/Politics/article/all/20210427/106642153/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106641976/1',\n",
       " 'https://www.donga.com/news/Inter/article/all/20210427/106631661/2',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106641813/1',\n",
       " 'https://www.donga.com/news/It/article/all/20210427/106641657/1',\n",
       " 'https://www.donga.com/news/list/article/all/20210427/106641639/2',\n",
       " 'https://www.donga.com/news/list/article/all/20210427/106641482/2',\n",
       " 'https://www.donga.com/news/list/article/all/20210427/106641425/2',\n",
       " 'https://www.donga.com/news/list/article/all/20210427/106641279/2',\n",
       " 'https://www.donga.com/news/Politics/article/all/20210427/106641244/1',\n",
       " 'https://www.donga.com/news/Society/article/all/20210427/106641235/1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#  경향닷컴 경제뉴스 RSS\n",
    "feeds = feedparser.parse('http://rss.donga.com/total.xml')\n",
    "print(type(feeds))\n",
    "print(feeds.keys())\n",
    "print(type(feeds['entries']))\n",
    "for entry in feeds['entries']:\n",
    "    print(entry.keys())\n",
    "    print('------------------')\n",
    "links = [entry['link'] for entry in feeds['entries']]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "임서원, 신곡 ‘어깨춤’으로 ‘화요 청백전’ 축하 무대…끼 폭발\n",
      "새벽 시간대 귀금속 매장 턴 일당 검거…1명 구속·3명 입건\n",
      "브레이브걸스 ‘비디오스타’서 예능감 발산…‘십장로’와 감동 재회\n",
      "‘나빌레라’ 송강, 최정상 발레리노 성장…박인환과 감동의 재회\n",
      "설운도, 이만기에 ‘빨대 꽂기’ 대결 승리…반전 결과\n",
      "유이 “살찌는 체형이라 다이어트 많이 해…과거 ‘꿀벅지’ 콤플렉스”\n",
      "옥주현, 특수 기계 활용한 운동법 공개…“무대 위에서 효과 느껴져”\n",
      "[사설]3년 전 판문점에 멈춘 文 인식으론 바이든 설득할 수 없다\n",
      "[사설]정치권 득실 계산 따라 자고나면 뒤집히는 與 종부세 정책\n",
      "[사설]아베보다도 더한 스가 정권의 독도·과거사 퇴행\n",
      "與, 합당으로 돌아오는 탈당자도 공천 감점…정봉주 “갈라치기 졸렬”\n",
      "헬멧 없이 5세 아이와 전동차 탑승 후…딸 잃은 母 눈물\n",
      "“마스크 써도 아이폰 잠금해제”…애플, 아이폰 새 운영체제 공개\n",
      "축제 즐기던 인도 거리…장례식장으로 변했다 [청계천 옆 사진관]\n",
      "‘코로나 폭증’ 인도 현지 상황 악화에 …정부 “부정기 항공편 허가”\n",
      "‘코로나 뉴 트렌드’…런던 금융가 빈 사무실 주택으로 개조\n",
      "정진석 추기경 선종… 향년 90세\n",
      "[단독]北 해산물·그림… 수출 금지 품목인데 온라인서 버젓이 판매\n",
      "군, 내일 성주 사드 기지로 물자 반입…반대단체와 충돌 우려\n",
      "날아 차기로 車 창문에 뛰어든 시민…‘광란 운전’ 멈춰 (영상)\n",
      "여야·정부, 종부세 대상 추산치 ‘제각각’…과세 앞두고 혼란 가중\n",
      "공수처 1호 사건 ‘검사 비위’ 될까…접수 사건 10건 중 4건 검사 관련\n",
      "화이자 추가 계약하자…‘혈전 논란 AZ 안 맞는다’ 노쇼 사태\n",
      "텐트치고 야영하던 부부 숨진 채 발견…일산화탄소 중독 추정\n",
      "불법 좌회전 택시 오토바이 치어 헬멧 안 쓴 고교생 사망\n",
      "“목욕탕 개업쇼인 줄” 명문대생 댄스 공연에 中 ‘시끌’ (영상)\n",
      "“짧아도 괜찮아”… 랜드로버코리아, ‘디펜더90’ 사전계약 돌입\n",
      "[리뷰] 버튼 누르면 PC가 위로 스윽, 루나랩 전동 스탠딩 책상\n",
      "달리는 택시서 흉기로 기사 위협한 여성…현행범 체포\n",
      "전자랜드, 4강 2연패 뒤 2연승…KCC에 21점 차 완승\n",
      "윤여정에 ‘냄새’ 질문한 매체, 사과 없이 ‘쓱’ 삭제 빈축\n",
      "부동산에 대선도 발목 잡힐라…與, 재산세·종부세 논의 서두른다\n",
      "경찰 “오세훈 ‘TBS 지원 중단’ 발언 방송법 위반 아니다”\n",
      "직진 차로서 불법 좌회전 택시, 오토바이 ‘쾅’…고교생 1명 사망\n",
      "‘부성 우선’ 폐지 추진…부부 협의땐 엄마姓 쓸 수 있게 바꾼다\n",
      "‘20억 낙찰’ 세상에서 가장 비싼 운동화…누구 신발?\n",
      "국방부 “28일 성주 사드기지에 발전기 등 지상수송”\n",
      "에쓰오일, 1분기 영업이익 6292억원 ‘깜짝 실적’… 최근 5년 최고 성적\n",
      "도쿄올림픽·패럴림픽 선수단 100명, 29일 화이자 백신 접종\n",
      "중수본 “대통령 5인 전직 참모 만찬은 사적모임 아닌 공무 성격”\n",
      "서울시, 물류센터·콜센터에 자가검사키트 시범 도입…구매비용 시 부담\n",
      "육상대회 1등은 개?…경기장 난입한 ‘우사인 댕댕이’ (영상)\n",
      "오세훈 ‘TBS 지원 중단’ 발언…경찰 “방송법 위반 아냐”\n",
      "한미약품, 1분기 매출 감소에도 영업益 4.2%↑… 개량·복합신약 내실성장 견인\n",
      "농협중앙회, 1분기 종합경영분석회의 개최\n",
      "농협, 2020 범농협 사회공헌보고서 발간\n",
      "쌍용차, 경영정상화 위한 조직개편 단행\n",
      "대우건설, 싱가포르 지하철 ‘크로스 아일랜드 라인 CR108 공구’ 수주\n",
      "尹 그늘에 가려졌던 야권 주자들 ‘꿈틀’…원희룡·유승민·홍준표 보폭 넓힌다\n",
      "사유리처럼 ‘비혼 출산’ 가능해질까…여성가족부, 의견 수렴 나선다\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'TV조선 ‘화요 청백전’ © 뉴스1\\n\\n임서원이 ‘화요 청백전’ 축하 무대를 꾸몄다.27일 오후 10시에 처음 방송된 TV조선 새 예능 프로그램 ‘화요 청백전’에서는 ‘스태미나’를 주제로 청팀(양지은, 별사랑, 은가은, 황우림, 이만기, 김용임, 김혜연)과 백팀(홍지윤, 김의영, 강혜연, 마리아, 설운도, 강진, 조혜련)의 대결이 펼쳐졌다. 이휘재 이찬원이 진행을, 박명수가 청팀 팀장, 홍현희가 백팀 팀장을 맡았다.1라운드 ‘빙글빙글 빨대 꽂기’(코끼리 코 15바퀴 후 요구르트에 빨대를 빨리 꽂는 게임)에서 백팀이 승리한 가운데, ‘트로트 공주’ 임서원의 축하 무대가 이어졌다.임서원이 신곡 ‘어깨춤’ 무대를 선보이며 시선을 모았다. 임서원은 중독성 강한 안무와 함께 청량한 목소리가 더해진 댄스곡으로 모두의 어깨를 들썩이게 했다. 임서원은 넘치는 끼를 폭발하며 모두의 흥을 돋우었다.한편, TV조선 ‘화요 청백전’은 ‘미스트롯 2’의 주역들과 다양한 분야의 게스트들이 청백전 구도로 대결을 펼치는 팀 버라이어티 게임 쇼로 매주 화요일 오후 10시에 방송된다.(서울=뉴스1)경기 용인동부경찰서는 새벽 시간대 금은방에 침입해 수백만 원 상당 귀금속을 훔친 혐의(특수절도)로 A(19)군을 구속해 검찰에 송치했다고 27일 밝혔다.범행을 도운 혐의(특수절도 방조 및 장물알선)로 B(20)씨 등 3명도 입건해 송치했다.A군은 13일 오전 5시께 용인시 처인구 한 금은방 유리 출입문을 둔기로 부순 뒤 들어가 진열장에 놓인 시계와 귀금속 등 513만 원 상당 금품을 훔친 혐의를 받고 있다.이튿날 인근 기흥구 소재 금거래소에도 같은 방법으로 침입을 시도했다 미수에 그친 혐의도 받고 있다.B씨 등은 A군에게 범행 장소와 도구를 물색·제공하고, 훔친 귀금속을 판매한 혐의를 받고 있다.경찰은 동일범 소행으로 추정되는 사건이 관내에서 잇따라 발생하자 전담 수사팀을 꾸려 용의자 추적에 나서 두 번째 범행 후 은신처에 숨어 있던 A군을 사건 발생 6시간 만에 검거했다.A군은 빚 독촉에 시달리자 '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import newspaper\n",
    "\n",
    "news_text =''\n",
    "for link in links:\n",
    "    article = newspaper.Article(link, language='ko')\n",
    "    article.download()\n",
    "    article.parse()\n",
    "    news_text += article.text\n",
    "    print(article.title)\n",
    "news_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(news_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from konlpy.tag import Mecab\n",
    "\n",
    "# engine = Mecab() \n",
    "# nouns = engine.nouns(news_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show jpype1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Twitter\n",
    "#from konlpy.tag import Okt\n",
    "\n",
    "nlpy = Twitter()\n",
    "#nlpy = Okt()\n",
    "nouns = nlpy.nouns(news_text)\n",
    "\n",
    "len(nouns)\n",
    "#print(nouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collections.Counter()\n",
    "* 동일한 값의 자료가 몇개인지를 파악하는데 사용하는 객체이다.\n",
    "* collections.Counter()의 결과값(return)은 딕셔너리 형태로 출력된다.\n",
    "<pre>\n",
    "lst = ['aa', 'cc', 'dd', 'aa', 'bb', 'ee']\n",
    "print(collections.Counter(lst))\n",
    "결과\n",
    "Counter({'aa': 2, 'cc': 1, 'dd': 1, 'bb': 1, 'ee': 1})\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "count = Counter(nouns)\n",
    "tags = count.most_common(40)\n",
    "tags[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip show wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#font_path = '/usr/share/fonts/truetype/nanum/NanumMyeongjoBold.ttf'\n",
    "font_path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "wc = WordCloud(font_path=font_path, background_color='white', width=800, height=600)\n",
    "cloud = wc.generate_from_frequencies(dict(tags))\n",
    "plt.figure(figsize=(10,8))\n",
    "plt.axis('off')\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import feedparser\n",
    "import newspaper\n",
    "from konlpy.tag import Okt\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "def draw_wordcloud_from_rss(rss_link):\n",
    "    try:\n",
    "        #  feedparser, newspaper: RSS를 통해 뉴스의 본문을 수집\n",
    "        feeds = feedparser.parse(rss_link)\n",
    "        links = [entry['link'] for entry in feeds['entries']]\n",
    "        print('links',len(links))\n",
    "        \n",
    "        news_text =''\n",
    "        for link in links:\n",
    "            article = newspaper.Article(link, language='ko')\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            news_text += article.text   \n",
    "        print('news_text',len(news_text))\n",
    "\n",
    "        # konlpy, Okt: 형태소 분석을 통해 본문에서 명사추출, 1글자는 단어는 삭제    \n",
    "        nlpy = Okt()\n",
    "        nouns = nlpy.nouns(news_text)\n",
    "\n",
    "        nouns = [n for n in nouns if len(n) > 1]\n",
    "        print('nouns',len(nouns))\n",
    "\n",
    "        # Counter: 단어수 세기, 가장 많이 등장한 단어(명사) 40개\n",
    "        count = Counter(nouns)\n",
    "        tags = count.most_common(40)\n",
    "        print('tags',tags[:20])\n",
    "\n",
    "        # WordCloud, matplotlib: 단어 구름 그리기\n",
    "        #font_path = '/usr/share/fonts/truetype/nanum/NanumMyeongjoBold.ttf'\n",
    "        font_path = \"c:/Windows/Fonts/malgun.ttf\"\n",
    "        wc = WordCloud(font_path=font_path, background_color='white', width=800, height=600)\n",
    "        cloud = wc.generate_from_frequencies(dict(tags))\n",
    "        plt.figure(figsize=(10,8))\n",
    "        plt.axis('off')\n",
    "        plt.imshow(cloud)\n",
    "    except (ValueError,KeyError):\n",
    "        #태그가 없을 경우 TypeError가 발생한다. \n",
    "        print(\"END\")\n",
    "        return None\n",
    "    \n",
    "# 경향신문 경제뉴스 RSS\n",
    "rss_link = 'http://www.khan.co.kr/rss/rssdata/politic_news.xml'\n",
    "\n",
    "draw_wordcloud_from_rss(rss_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import bs4\n",
    "import re\n",
    "\n",
    "fp = open('data/news_url.xml','r', encoding='utf-8')\n",
    "soup = BeautifulSoup(fp,'html.parser')\n",
    "\n",
    "for media in soup.find(\"media_list\", {\"name\":\"동아일보\"}):\n",
    "    if isinstance(media, bs4.element.Tag):\n",
    "        print(type(media),media)\n",
    "        for x in media.find_all(text=lambda tag: isinstance(tag, bs4.CData)):\n",
    "            result = x.string.strip()\n",
    "            print(result)\n",
    "            #draw_wordcloud_from_rss(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def select_newspaper(newspaper_name):\n",
    "    fp = open('data/news_url.xml','r', encoding='utf-8')\n",
    "    soup = BeautifulSoup(fp,'html.parser')\n",
    "    \n",
    "    for media in soup.find(\"media_list\", {\"name\":newspaper_name}):\n",
    "        if isinstance(media, bs4.element.Tag):\n",
    "            #print(type(media),media)\n",
    "            print(media['name'])\n",
    "            for x in media.find_all(text=lambda tag: isinstance(tag, bs4.CData)):\n",
    "                result = x.string.strip()\n",
    "                print(result)\n",
    "                draw_wordcloud_from_rss(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_newspaper('오마이뉴스')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
